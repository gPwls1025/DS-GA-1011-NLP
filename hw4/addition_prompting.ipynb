{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import together\n",
    "from time import sleep\n",
    "import re\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dprint(s, debug):\n",
    "    if debug:\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 9e3c2abd5e605c5f5d36f6760aefb82246de307313704f78884784dc149537d0\n",
    "# https://api.together.xyz/settings/api-keys\n",
    "YOUR_API_KEY = '9e3c2abd5e605c5f5d36f6760aefb82246de307313704f78884784dc149537d0'\n",
    "together.api_key = YOUR_API_KEY\n",
    "\n",
    "def call_together_api(prompt, student_configs, post_processing, model='togethercomputer/llama-2-7b', debug=False):\n",
    "    output = together.Complete.create(\n",
    "    prompt = prompt,\n",
    "    model = model, \n",
    "    **student_configs\n",
    "    )\n",
    "    dprint('*****prompt*****', debug)\n",
    "    dprint(prompt, debug)\n",
    "    dprint('*****result*****', debug)\n",
    "    res = output['output']['choices'][0]['text']\n",
    "    dprint(res, debug)\n",
    "    dprint('*****output*****', debug)\n",
    "    numbers_only = post_processing(res)\n",
    "    dprint(numbers_only, debug)\n",
    "    dprint('=========', debug)\n",
    "    return numbers_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Part 1. Zero Shot Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_addition_pairs(lower_bound, upper_bound, rng):\n",
    "    int_a = int(np.ceil(rng.uniform(lower_bound, upper_bound)))\n",
    "    int_b = int(np.ceil(rng.uniform(lower_bound, upper_bound)))\n",
    "    return int_a, int_b\n",
    "\n",
    "def test_range(added_prompt, prompt_configs, rng, n_sample=30, \n",
    "               lower_bound=1, upper_bound=10, fixed_pairs=None, \n",
    "               pre_processing=lambda x:x, post_processing=lambda y:y,\n",
    "               model='togethercomputer/llama-2-7b', debug=False):\n",
    "    int_as = []\n",
    "    int_bs = []\n",
    "    answers = []\n",
    "    model_responses = []\n",
    "    correct = []\n",
    "    prompts = []\n",
    "    iterations = range(n_sample) if fixed_pairs is None else fixed_pairs\n",
    "    for i, v in enumerate(tqdm(iterations)):\n",
    "        if fixed_pairs is None:\n",
    "            int_a, int_b = get_addition_pairs(lower_bound=lower_bound, upper_bound=upper_bound, rng=rng)\n",
    "        else:\n",
    "            int_a, int_b = v\n",
    "        fixed_prompt = f'{int_a}+{int_b}'\n",
    "        fixed_prompt = pre_processing(fixed_prompt)\n",
    "        prefix, suffix = added_prompt\n",
    "        prompt = prefix + fixed_prompt + suffix\n",
    "        model_response = call_together_api(prompt, prompt_configs, post_processing, model=model, debug=debug)\n",
    "        answer = int_a + int_b\n",
    "        int_as.append(int_a)\n",
    "        int_bs.append(int_b)\n",
    "        prompts.append(prompt)\n",
    "        answers.append(answer)\n",
    "        model_responses.append(model_response)\n",
    "        correct.append((answer == model_response))\n",
    "        sleep(1) # pause to not trigger DDoS defense\n",
    "    df = pd.DataFrame({'int_a': int_as, 'int_b': int_bs, 'prompt': prompts, 'answer': answers, 'response': model_responses, 'correct': correct})\n",
    "    print(df)\n",
    "    mae = mean_absolute_error(df['answer'], df['response'])\n",
    "    acc = df.correct.sum()/len(df)\n",
    "    prompt_length = len(prefix) + len(suffix)\n",
    "    res = acc * 1/prompt_length * (1-mae/(5*10^6))\n",
    "    return {'res': res, 'acc': acc, 'mae': mae, 'prompt_length': prompt_length}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    'huggyllama/llama-65b', #LLaMa-65B\n",
    "    'togethercomputer/llama-2-7b', #LLaMa-2-7B\n",
    "    'togethercomputer/llama-2-13b', #LLaMa-2-13B\n",
    "    'togethercomputer/llama-2-70b', #LLaMa-2-70B\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Zero-shot single-digit addition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_prompt = ('Question: What is ', '?\\nAnswer: ') # Question: What is a+b?\\nAnswer:\n",
    "prompt_config = {'max_tokens': 2,\n",
    "                'temperature': 0.7,\n",
    "                'top_k': 50,\n",
    "                'top_p': 0.6,\n",
    "                'repetition_penalty': 1,\n",
    "                'stop': []}\n",
    "\n",
    "# input_string: 'a+b'\n",
    "def your_pre_processing(input_string):\n",
    "    return input_string\n",
    "\n",
    "# output_string: \n",
    "# depending on your prompt, it might look like 'output: number'\n",
    "def your_post_processing(output_string):\n",
    "    # using regular expression to find the first consecutive digits in the returned string\n",
    "    only_digits = re.sub(r\"\\D\", \"\", output_string)\n",
    "    try:\n",
    "        res = int(only_digits)\n",
    "    except:\n",
    "        res = 0\n",
    "    return res\n",
    "\n",
    "model = 'togethercomputer/llama-2-7b'\n",
    "print(model)\n",
    "seed = 0\n",
    "rng = np.random.default_rng(seed)\n",
    "res = test_range(added_prompt=added_prompt, prompt_configs=prompt_config, rng=rng, n_sample=10, lower_bound=1, upper_bound=10, fixed_pairs=None, pre_processing=your_pre_processing, post_processing=your_post_processing, model=model, debug=False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Zero-shot 7-digit addition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(1) # wait a little bit to prevent api call error\n",
    "prompt_config['max_tokens'] = 8\n",
    "rng = np.random.default_rng(seed)\n",
    "res = test_range(added_prompt=added_prompt, prompt_configs=prompt_config, rng=rng, n_sample=10, lower_bound=1000000, upper_bound=9999999, fixed_pairs=None, pre_processing=your_pre_processing, post_processing=your_post_processing, model=model, debug=False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1a.** In your opinion, what are some factors that cause language model performance to deteriorate from 1 digit to 7 digits?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1b**. Play around with the config parameters ('max_tokens','temperature','top_k','top_p','repetition_penalty') in together.ai's [web UI](https://api.together.xyz/playground/language/togethercomputer/llama-2-7b). \n",
    "* What does each parameter represent?\n",
    "* How does increasing each parameter change the generation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1c**. Do 7-digit addition with 70B parameter llama model. \n",
    "* How does the performance change?\n",
    "* What are some factors that cause this change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(1) # wait a little bit to prevent api call error\n",
    "rng = np.random.default_rng(seed)\n",
    "res = test_range(added_prompt=added_prompt, prompt_configs=prompt_config, rng=rng, n_sample=10, lower_bound=1000000, upper_bound=9999999, fixed_pairs=None, pre_processing=your_pre_processing, post_processing=your_post_processing, model='togethercomputer/llama-2-70b', debug=False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1d.** Here we're giving our language model the prior that the sum of two 7-digit numbers must have a maximum of 8 digits. (by setting max_token=8). What if we remove this prior by increasing the max_token to 20? \n",
    "* Does the model still perform well?\n",
    "* What are some reasons why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(1) # wait a little bit to prevent api call error\n",
    "added_prompt = ('Question: What is ', '?\\nAnswer: ') # Question: What is a+b?\\nAnswer:\n",
    "prompt_config = {'max_tokens': 20,\n",
    "                'temperature': 0.7,\n",
    "                'top_k': 50,\n",
    "                'top_p': 0.6,\n",
    "                'repetition_penalty': 1,\n",
    "                'stop': []}\n",
    "\n",
    "# input_string: 'a+b'\n",
    "def your_pre_processing(input_string):\n",
    "    return input_string\n",
    "\n",
    "def your_post_processing(output_string):\n",
    "    first_line = output_string.splitlines()[0]\n",
    "    only_digits = re.sub(r\"\\D\", \"\", first_line)\n",
    "    try:\n",
    "        res = int(only_digits)\n",
    "    except:\n",
    "        res = 0\n",
    "    return res\n",
    "\n",
    "\n",
    "model = 'togethercomputer/llama-2-7b'\n",
    "print(model)\n",
    "seed = 0\n",
    "rng = np.random.default_rng(seed)\n",
    "res = test_range(added_prompt=added_prompt, prompt_configs=prompt_config, rng=rng, n_sample=10, lower_bound=1000000, upper_bound=9999999, fixed_pairs=None, pre_processing=your_pre_processing, post_processing=your_post_processing, model=model, debug=False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. In Context Learning\n",
    "\n",
    "We will try to improve the performance of 7-digit addition via in-context learning.\n",
    "For cost-control purposes (you only have $25 free credits), we will use [llama-2-7b](https://api.together.xyz/playground/language/togethercomputer/llama-2-7b). Below is a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(1) # wait a little bit to prevent api call error\n",
    "added_prompt = ('Question: What is 3+7?\\nAnswer: 10\\n Question: What is ', '?\\nAnswer: ') # Question: What is a+b?\\nAnswer:\n",
    "prompt_config = {'max_tokens': 8,\n",
    "                'temperature': 0.7,\n",
    "                'top_k': 50,\n",
    "                'top_p': 0.6,\n",
    "                'repetition_penalty': 1,\n",
    "                'stop': []}\n",
    "rng = np.random.default_rng(seed)\n",
    "res = test_range(added_prompt=added_prompt, prompt_configs=prompt_config, rng=rng, n_sample=10, lower_bound=1000000, upper_bound=9999999, fixed_pairs=None, pre_processing=your_pre_processing, post_processing=your_post_processing, model='togethercomputer/llama-2-7b', debug=False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2a**.\n",
    "* How does the performance change with the baseline in-context learning prompt? (compare with \"Example: Zero-shot 7-digit addition\" in Q1)\n",
    "* What are some factors that cause this change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will remove the prior on output length and re-evaluate the performance of our baseline one-shot learning prompt. We need to modify our post processing function to extract the answer from the output sequence. In this case, it is the number in the first line that starts with \"Answer: \"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2b**.\n",
    "* How does the performance change when we relax the output length constraint? (compare with Q2a)\n",
    "* What are some factors that cause this change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(1) # wait a little bit to prevent api call error\n",
    "\n",
    "prompt_config['max_tokens'] = 50 # changed from 8, assuming we don't know the output length\n",
    "                \n",
    "rng = np.random.default_rng(seed)\n",
    "res = test_range(added_prompt=added_prompt, prompt_configs=prompt_config, rng=rng, n_sample=10, lower_bound=1000000, upper_bound=9999999, fixed_pairs=None, pre_processing=your_pre_processing, post_processing=your_post_processing, model='togethercomputer/llama-2-7b', debug=False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2c.** Let's change our one-shot learning example to something more \"in-distribution\". Previously we were using 1-digit addition as an example. Let's change it to 7-digit addition (1234567+1234567=2469134). \n",
    "* Evaluate the performance with max_tokens = 8.\n",
    "* Evaluate the performance with max_tokens = 50.\n",
    "* How does the performance change from 1-digit example to 7-digit example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(1) # wait a little bit to prevent api call error\n",
    "prompt_config['max_tokens'] = 8 \n",
    "added_prompt = ('Question: What is 1234567+123457?\\nAnswer: 2469134\\nQuestion: What is ', '?\\nAnswer: ') # Question: What is a+b?\\nAnswer:\n",
    "test_range(added_prompt=added_prompt, prompt_configs=prompt_config, rng=rng, n_sample=10, lower_bound=1000000, upper_bound=9999999, fixed_pairs=None, pre_processing=your_pre_processing, post_processing=your_post_processing, model='togethercomputer/llama-2-7b', debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(1) # wait a little bit to prevent api call error\n",
    "prompt_config['max_tokens'] = 50 \n",
    "test_range(added_prompt=added_prompt, prompt_configs=prompt_config, rng=rng, n_sample=10, lower_bound=1000000, upper_bound=9999999, fixed_pairs=None, pre_processing=your_pre_processing, post_processing=your_post_processing, model='togethercomputer/llama-2-7b', debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2d.** Let's look at a specific example with large absolute error. \n",
    "* Run the cell at least 5 times. Does the error change with each time? Why?\n",
    "* Can you think of a prompt to reduce the error?\n",
    "* Why do you think it would work?\n",
    "* Does it work in practice? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_range(added_prompt=added_prompt, prompt_configs=prompt_config, rng=rng, fixed_pairs=[(9090909,1010101)], pre_processing=your_pre_processing, post_processing=your_post_processing, model='togethercomputer/llama-2-7b', debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Prompt-a-thon (autograder & leaderboard)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compete with your classmates to see who's best at teach llama to add 7-digit numbers reliably! Submit your ```submission.py``` to enter the leader board!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
